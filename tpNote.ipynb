{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importer les données et les traiter pour que tout soit numérique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "\n",
    "data = pd.read_csv(\"Obesite_train.csv\", sep=\";\")\n",
    "\n",
    "# pour voir les données vite fait\n",
    "#print(data.head())\n",
    "\n",
    "X = data.iloc[:, :-1].values  # toutes les colonnes à savoir\n",
    "y = data.iloc[:, -1].values  # la colonne à prédire\n",
    "\n",
    "\n",
    "# variables catégorielles \n",
    "col_cat = [1, 5, 6, 9, 10, 12, 15, 16]\n",
    "X_cat = np.copy(X[:, col_cat])  \n",
    "for col_id in range(len(col_cat)): \n",
    "    unique_val, val_idx = np.unique(X_cat[:, col_id], return_inverse=True) \n",
    "    X_cat[:, col_id] = val_idx \n",
    "imp_cat = SimpleImputer (missing_values=0, strategy='most_frequent') \n",
    "X_cat[:, range(2)] = imp_cat.fit_transform(X_cat[:, range(2)]) \n",
    "\n",
    "\n",
    "# variables numériques \n",
    "col_num = [0, 2, 3, 4, 7, 8, 11, 13, 14]\n",
    "X_num = np.copy(X[:, col_num]) \n",
    "X_num[X_num == '?'] = np.nan \n",
    "X_num = X_num.astype(float) \n",
    "imp_num = SimpleImputer (missing_values=np.nan, strategy='mean') \n",
    "X_num = imp_num.fit_transform(X_num) \n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "X_cat_bin = encoder.fit_transform(X_cat).toarray() \n",
    "\n",
    "\n",
    "# on concatène les variables catégorielles et numériques\n",
    "X = np.hstack((X_cat_bin, X_num)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on sépare les données en train et test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on cherche le meilleur classifieur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import recall_score, make_scorer\n",
    "\n",
    "# Liste des modèles\n",
    "\n",
    "# NaiveBayesSimple \n",
    "# Un arbre CART \n",
    "# Un arbre ID3 \n",
    "# Un Decision Stump \n",
    "# MultilayerPerceptron à deux couches de tailles respectives 20 et 10 par exemple \n",
    "# k-plus-proches-voisins avec k=5 par exemple \n",
    "# Bagging avec 200 classifieurs par exemple \n",
    "# AdaBoost avec 200 classifieurs par exemple \n",
    "# Random Forest avec 200 classifieurs par exemple \n",
    "# Un XGboost avec 200 classifieurs par exemple\n",
    "\n",
    "models = {\n",
    "    'NB': GaussianNB(),\n",
    "    'CART': DecisionTreeClassifier(random_state=1),\n",
    "    'DS': DecisionTreeClassifier(max_depth=1, random_state=1),\n",
    "    'MLP': MLPClassifier(hidden_layer_sizes=(40, 40, 20, 10), random_state=1),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'BAG':BaggingClassifier(n_estimators=200,  random_state=1),\n",
    "    'RF': RandomForestClassifier(n_estimators=200, random_state=1),\n",
    "    'AB': AdaBoostClassifier(n_estimators=200, random_state=1),\n",
    "    'XGB': GradientBoostingClassifier(n_estimators=200, random_state=1)\n",
    "}\n",
    "\n",
    "def custom_cost(y_true, y_pred):\n",
    "    recall_0 = recall_score(y_true, y_pred, pos_label=0) \n",
    "    recall_1 = recall_score(y_true, y_pred, pos_label=1)\n",
    "    return 0.5 * recall_0 + 0.5 * recall_1\n",
    "\n",
    "# création du scorer (pour personnaliser ton truc à maximiser)\n",
    "# dedans je mets la fonction du score donnée dans l'énoncé\n",
    "scorer = make_scorer(custom_cost, greater_is_better=True)\n",
    "\n",
    "# fonction d'évaluation des modèles    \n",
    "    \n",
    "def run_classifiers(clfs, X, Y):\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "    for i in clfs:\n",
    "        clf = clfs[i]\n",
    "        cv_acc = cross_val_score(clf, X, Y, cv=kf, scoring=scorer)\n",
    "        print(\"Accuracy for {0} is: {1:.3f} +/- {2:.3f}\".format(i, np.mean(cv_acc), np.std(cv_acc)))\n",
    "        \n",
    "\n",
    "print(\"on compare les modèles (data d'origine)\")\n",
    "run_classifiers(models, X, y)\n",
    "\n",
    "#data normalization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "print(\"on compare les modèles (data normalisé)\")\n",
    "run_classifiers(models, X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on trouve que le meilleur classifieur est AB donc on cherche ses meilleurs paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Définir les paramètres à optimiser pour AB\n",
    "parametres_a_optimiser = {\n",
    "    \"n_estimators\": [50, 100, 200, 400],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1, 0.2, 0.5],\n",
    "    \"random_state\": [1]  # ne bouge pas\n",
    "}\n",
    "\n",
    "# Instancier GridSearchCV pour trouver les meilleurs paramètres\n",
    "grid_search = GridSearchCV(\n",
    "    AdaBoostClassifier(),\n",
    "    parametres_a_optimiser,\n",
    "    scoring=scorer,\n",
    "    cv=3,  # Validation croisée à 3 plis\n",
    "    n_jobs=-1,  # Utiliser tous les cœurs disponibles\n",
    "    verbose=1  # Affiche les détails de l'optimisation\n",
    ")\n",
    "\n",
    "\n",
    "# Ajuster le modèle sur les données d'entraînement\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Meilleurs paramètres :\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on obtient ça : \"Meilleurs paramètres : {'learning_rate': 0.1, 'n_estimators': 100, 'random_state': 1}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# du coup full training sur le meilleur modele \n",
    "final_model = AdaBoostClassifier(learning_rate =0.1, n_estimators=100, random_state=1)\n",
    "final_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "et maintenant on peut faire les prédictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final = pd.read_csv(\"Obesite_scoring.csv\", sep=\";\")\n",
    "\n",
    "# on applique les mêmes transformations que pour les données d'entraînement\n",
    "\n",
    "# variables catégorielles\n",
    "X_scoring_cat = np.copy(final.iloc[:, col_cat].values)\n",
    "for col_id in range(len(col_cat)):\n",
    "    unique_val, val_idx = np.unique(X_scoring_cat[:, col_id], return_inverse=True)\n",
    "    X_scoring_cat[:, col_id] = val_idx\n",
    "X_scoring_cat[:, range(2)] = imp_cat.transform(X_scoring_cat[:, range(2)])\n",
    "\n",
    "\n",
    "# variables numériques\n",
    "X_scoring_num = np.copy(final.iloc[:, col_num].values)\n",
    "X_scoring_num[X_scoring_num == '?'] = np.nan\n",
    "X_scoring_num = X_scoring_num.astype(float)\n",
    "X_scoring_num = imp_num.transform(X_scoring_num)\n",
    "\n",
    "X_scoring_cat_bin = encoder.transform(X_scoring_cat).toarray()\n",
    "X_scoring_prepared = np.hstack((X_scoring_cat_bin, X_scoring_num))\n",
    "\n",
    "# Prédiction sur le fichier de scoring\n",
    "y_scoring_pred = final_model.predict(X_scoring_prepared)\n",
    "\n",
    "# Création du fichier de sortie avec Id et classe\n",
    "scoring_results = pd.DataFrame({\n",
    "    \"Id\": final[\"Id\"],  \n",
    "    \"Obesite?\": y_scoring_pred\n",
    "})\n",
    "\n",
    "# on sauvegarde dans un fichier csv\n",
    "scoring_results.to_csv(\"Obesite_scoring_predictions.csv\", index=False, sep=\";\", columns=[\"Id\", \"Obesite?\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
